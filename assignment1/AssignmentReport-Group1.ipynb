{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![task 1](Task1.a.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Calculous to makes the derivative more easily.\n",
    "\n",
    "![task 1b](Task.b1.png)\n",
    "\n",
    "The derivative of a function g(w) is the following:\n",
    "\n",
    "![task 2b](Task.b2.png)\n",
    "\n",
    "The derivative of a function h(w) is the following:\n",
    "\n",
    "![task 3b](Task.b3.png)\n",
    "\n",
    "Now we merge the derivative of g(w) with the derivative of h(w) and we get the following:\n",
    "\n",
    "![task 4b](Task.b4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "In our test, we were bumbed out after 17 epochs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "Input is more stable, the model is trained on different pictures compared to the immediatly validatd ones\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "In Figure 6 the training accuracy is better than the validation accuracy after 3000 training steps. And the increase distance between the training accuracy and the validation accuracy could be a sign of overfitting, because the model perfomrs better in one than the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "![Task 4a](Task4a.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4b_softmax_weight.png)\n",
    "\n",
    "With stronger L2 normalization (high values for lambda), the model tries to be simpler, the difference within different values would be lower, than the noise is reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)\n",
    "\n",
    "Since L2 normalization penalize high values, the model tries to reduce them. It brings to a worse fitting model since it wouldn't align to the points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "On increasing lambda, l2 value lowers down. This is probabilly caused by the penalization of L2 on higher lambda values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "df634a05ea98b8e51c3f9b1ec71d257a08b6489e2fa4ba6256066b04a3fcb8c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
