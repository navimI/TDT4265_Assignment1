{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a15bb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "from tops.config import LazyConfig\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Include ../SSD in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faed6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lhome/ivang/tdt4265/TDT4265_Assignment1/assignment4/SSD\n"
     ]
    }
   ],
   "source": [
    "print(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a16d28",
   "metadata": {},
   "source": [
    "# Introduction to SSD\n",
    "\n",
    "This code is much more complex than your prior assignment, and we recommend you to spend some time getting familiar with the code structure.\n",
    "The \"complex\" code structure is made to simplify aspects of deep learning experimentation, and help you structure the usual \"sphagetti\" deep learning code.\n",
    "\n",
    "\n",
    "All scripts in this code requires a configuration file. To **start training**, you can type:\n",
    "```\n",
    "python train.py configs/ssd300.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ea8e3",
   "metadata": {},
   "source": [
    "## Configuration files\n",
    "The key difference from previous starter codes is the use of configuration files. This enables us to change small parts of the experiment without changing hard-coded values (e.g. learning rate in previous assignments).\n",
    "\n",
    "If you take a look in [`configs/ssd300.py`](../configs/ssd300.py) you will notice a large set of objects describing model architecture (`backbone` and `model`), the optimizer, dataset, data loading, and hyperparameters.\n",
    "\n",
    "To load the config we can write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66214f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SSD outputs to: outputs/\n"
     ]
    }
   ],
   "source": [
    "from ssd.utils import load_config\n",
    "#cfg = load_config(\"../configs/ssd300.py\")\n",
    "path = \"../configs/ssd300.py\"\n",
    "\n",
    "config_path = Path(path)\n",
    "run_name = \"_\".join(config_path.parts[1:-1]) + \"_\" + config_path.stem\n",
    "cfg = LazyConfig.load(os.fspath(config_path))\n",
    "cfg.output_dir = Path(cfg.train._output_dir).joinpath(*config_path.parts[1:-1], config_path.stem)\n",
    "cfg.run_name = run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a680c",
   "metadata": {},
   "source": [
    "`cfg` supports access syntax, where all objects in `configs/ssd300.py` are accessible via their attribute name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ddc89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extractor': '${backbone}', 'anchors': '${anchors}', 'loss_objective': '${loss_objective}', 'num_classes': 11, '_target_': <class 'ssd.modeling.ssd.SSD300'>}\n"
     ]
    }
   ],
   "source": [
    "print(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7b97d7",
   "metadata": {},
   "source": [
    "If we print `cfg.model`, notice that it returns a dictionary and not the model object itself (which is `SSD300` from [`ssd/modeling/ssd.py`](../ssd/modeling/ssd.py)). This is because the model is defined \"lazily\" (wrapped with a `LazyCall`).\n",
    "\n",
    "To create the model, we have to instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8daf7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD300(\n",
      "  (feature_extractor): BasicModel(\n",
      "    (features): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): ReLU()\n",
      "        (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (9): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): ReLU()\n",
      "        (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss_func): SSDMultiboxLoss(\n",
      "    (sl1_loss): SmoothL1Loss()\n",
      "  )\n",
      "  (regression_heads): ModuleList(\n",
      "    (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2-3): 2 x Conv2d(128, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4-5): 2 x Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (classification_heads): ModuleList(\n",
      "    (0): Conv2d(128, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2-3): 2 x Conv2d(128, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4-5): 2 x Conv2d(64, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from tops.config import instantiate\n",
    "model = instantiate(cfg.model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67edd3a",
   "metadata": {},
   "source": [
    "Another example, we can load the first batch of the dataset and run a forward pass with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af819ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "image has the shape: torch.Size([32, 3, 300, 300])\n",
      "boxes has the shape: torch.Size([32, 8732, 4])\n",
      "labels has the shape: torch.Size([32, 8732])\n",
      "The model predicted anchors with  bbox delta: torch.Size([32, 4, 8732]) and confidences: torch.Size([32, 11, 8732])\n"
     ]
    }
   ],
   "source": [
    "dataloader_train = instantiate(cfg.data_train.dataloader)\n",
    "print(type(dataloader_train))\n",
    "batch = next(iter(dataloader_train))\n",
    "for key, item in batch.items():\n",
    "    print(key, \"has the shape:\", item.shape)\n",
    "gpu_transform = instantiate(cfg.data_train.gpu_transform)\n",
    "batch = gpu_transform(batch)\n",
    "bbox_delta, confidences = model(batch[\"image\"])\n",
    "print(f\"The model predicted anchors with  bbox delta: {bbox_delta.shape} and confidences: {confidences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f15dce",
   "metadata": {},
   "source": [
    "You might ask yourself, why? At first sight, this seems very complicated rather than plain-old  hard coded values.\n",
    "\n",
    "The reason is easy manipulation of experiments. If I want to run the same experiment, but with a different batch size, I can change it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02b474c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original batch size: 2\n",
      "New batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Lets print the batch size of the original data loader:\n",
    "print(\"Original batch size:\", dataloader_train.batch_size)\n",
    "cfg.train.batch_size = 2 # Setting the batch size to 2\n",
    "dataloader_train = instantiate(cfg.data_train.dataloader)\n",
    "print(\"New batch size:\", dataloader_train.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120d29c",
   "metadata": {},
   "source": [
    "Another reason is **configuration inheritance**. E.g. for the last task, you are going to train VGG on the VOC dataset. \n",
    "This requires us to change the backbone and dataset, however, keep all other parameters the same (e.g. general configs (cfg.train), anchors, schedulers, optimizers, etc.).\n",
    "\n",
    "Take a look in [`configs/voc_vgg.py`](../configs/voc_vgg.py) and notice that we inherit from the original config:\n",
    "```\n",
    "from .ssd300 import train, anchors, optimizer, schedulers, model, data_train, data_val\n",
    "```\n",
    "The only changes done are to the backbone, dataset and dataset transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "655e002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SSD outputs to: outputs/\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2007\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2012\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2007\n",
      "500\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD300(\n",
      "  (feature_extractor): VGG(\n",
      "    (vgg): ModuleList(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace=True)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace=True)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace=True)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace=True)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace=True)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace=True)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace=True)\n",
      "    )\n",
      "    (extras): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "        (2): ReLU()\n",
      "        (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): ReLU()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (3-4): 2 x Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (l2_norm): L2Norm()\n",
      "  )\n",
      "  (loss_func): SSDMultiboxLoss(\n",
      "    (sl1_loss): SmoothL1Loss()\n",
      "  )\n",
      "  (regression_heads): ModuleList(\n",
      "    (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (classification_heads): ModuleList(\n",
      "    (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4-5): 2 x Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "path = \"../configs/voc_vgg.py\"\n",
    "\n",
    "config_path = Path(path)\n",
    "run_name = \"_\".join(config_path.parts[1:-1]) + \"_\" + config_path.stem\n",
    "cfg = LazyConfig.load(os.fspath(config_path))\n",
    "cfg.output_dir = Path(cfg.train._output_dir).joinpath(*config_path.parts[1:-1], config_path.stem)\n",
    "cfg.run_name = run_name\n",
    "print(cfg.schedulers.linear.total_iters)\n",
    "cfg.schedulers.linear.total_iters = 5000\n",
    "print(cfg.schedulers.linear.total_iters)\n",
    "model = instantiate(cfg.model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e53a0a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-24 19:12:33.427085: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Saving SSD outputs to: outputs/\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2007\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2012\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2007\n",
      "--------------------Config file below--------------------\n",
      "{ 'anchors': { '_target_': <class 'ssd.modeling.anchor_boxes.AnchorBoxes'>,\n",
      "               'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
      "               'feature_sizes': [ [38, 38],\n",
      "                                  [19, 19],\n",
      "                                  [10, 10],\n",
      "                                  [5, 5],\n",
      "                                  [3, 3],\n",
      "                                  [1, 1]],\n",
      "               'image_shape': '${train.imshape}',\n",
      "               'min_sizes': [ [15, 15],\n",
      "                              [30, 30],\n",
      "                              [111, 111],\n",
      "                              [162, 162],\n",
      "                              [213, 213],\n",
      "                              [264, 264],\n",
      "                              [315, 315]],\n",
      "               'scale_center_variance': 0.1,\n",
      "               'scale_size_variance': 0.2,\n",
      "               'strides': [ [8, 8],\n",
      "                            [16, 16],\n",
      "                            [32, 32],\n",
      "                            [64, 64],\n",
      "                            [100, 100],\n",
      "                            [300, 300]]},\n",
      "  'data_train': { 'dataloader': { '_target_': <class 'torch.utils.data.dataloader.DataLoader'>,\n",
      "                                  'batch_size': '${...train.batch_size}',\n",
      "                                  'collate_fn': <function batch_collate at 0x7f23e4e350d0>,\n",
      "                                  'dataset': '${..dataset}',\n",
      "                                  'drop_last': True,\n",
      "                                  'num_workers': 4,\n",
      "                                  'pin_memory': True,\n",
      "                                  'shuffle': True},\n",
      "                  'dataset': { '_target_': <class 'torch.utils.data.dataset.ConcatDataset'>,\n",
      "                               'datasets': [ { '_target_': <class 'ssd.data.voc.VOCDataset'>,\n",
      "                                               'data_dir': '/work/datasets/VOCdevkit/VOC2007',\n",
      "                                               'keep_difficult': True,\n",
      "                                               'remove_empty': True,\n",
      "                                               'split': 'train',\n",
      "                                               'transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                                                              'transforms': [ { '_target_': <class 'ssd.data.transforms.transform.RandomSampleCrop'>},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.transform.ToTensor'>},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.transform.RandomHorizontalFlip'>},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.transform.Resize'>,\n",
      "                                                                                'imshape': '${train.imshape}'},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.target_transform.GroundTruthBoxesToAnchors'>,\n",
      "                                                                                'anchors': '${anchors}',\n",
      "                                                                                'iou_threshold': 0.5}]}},\n",
      "                                             { '_target_': <class 'ssd.data.voc.VOCDataset'>,\n",
      "                                               'data_dir': '/work/datasets/VOCdevkit/VOC2012',\n",
      "                                               'keep_difficult': True,\n",
      "                                               'remove_empty': True,\n",
      "                                               'split': 'train',\n",
      "                                               'transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                                                              'transforms': [ { '_target_': <class 'ssd.data.transforms.transform.RandomSampleCrop'>},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.transform.ToTensor'>},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.transform.RandomHorizontalFlip'>},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.transform.Resize'>,\n",
      "                                                                                'imshape': '${train.imshape}'},\n",
      "                                                                              { '_target_': <class 'ssd.data.transforms.target_transform.GroundTruthBoxesToAnchors'>,\n",
      "                                                                                'anchors': '${anchors}',\n",
      "                                                                                'iou_threshold': 0.5}]}}]},\n",
      "                  'gpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                                     'transforms': [ { '_target_': <class 'ssd.data.transforms.gpu_transforms.Normalize'>,\n",
      "                                                       'mean': [ 0.485,\n",
      "                                                                 0.456,\n",
      "                                                                 0.406],\n",
      "                                                       'std': [ 0.229,\n",
      "                                                                0.224,\n",
      "                                                                0.225]}]}},\n",
      "  'data_val': { 'dataloader': { '_target_': <class 'torch.utils.data.dataloader.DataLoader'>,\n",
      "                                'batch_size': '${...train.batch_size}',\n",
      "                                'collate_fn': <function batch_collate_val at 0x7f23e4e35160>,\n",
      "                                'dataset': '${..dataset}',\n",
      "                                'num_workers': 4,\n",
      "                                'pin_memory': True,\n",
      "                                'shuffle': False},\n",
      "                'dataset': { '_target_': <class 'ssd.data.voc.VOCDataset'>,\n",
      "                             'data_dir': '/work/datasets/VOCdevkit/VOC2007',\n",
      "                             'remove_empty': False,\n",
      "                             'split': 'val',\n",
      "                             'transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                                            'transforms': [ { '_target_': <class 'ssd.data.transforms.transform.ToTensor'>},\n",
      "                                                            { '_target_': <class 'ssd.data.transforms.transform.Resize'>,\n",
      "                                                              'imshape': '${train.imshape}'}]}},\n",
      "                'gpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                                   'transforms': [ { '_target_': <class 'ssd.data.transforms.gpu_transforms.Normalize'>,\n",
      "                                                     'mean': [ 0.485,\n",
      "                                                               0.456,\n",
      "                                                               0.406],\n",
      "                                                     'std': [ 0.229,\n",
      "                                                              0.224,\n",
      "                                                              0.225]}]}},\n",
      "  'gpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                     'transforms': [ { '_target_': <class 'ssd.data.transforms.gpu_transforms.Normalize'>,\n",
      "                                       'mean': [0.485, 0.456, 0.406],\n",
      "                                       'std': [0.229, 0.224, 0.225]}]},\n",
      "  'label_map': { 0: '__background__',\n",
      "                 1: 'aeroplane',\n",
      "                 2: 'bicycle',\n",
      "                 3: 'bird',\n",
      "                 4: 'boat',\n",
      "                 5: 'bottle',\n",
      "                 6: 'bus',\n",
      "                 7: 'car',\n",
      "                 8: 'cat',\n",
      "                 9: 'chair',\n",
      "                 10: 'cow',\n",
      "                 11: 'diningtable',\n",
      "                 12: 'dog',\n",
      "                 13: 'horse',\n",
      "                 14: 'motorbike',\n",
      "                 15: 'person',\n",
      "                 16: 'pottedplant',\n",
      "                 17: 'sheep',\n",
      "                 18: 'sofa',\n",
      "                 19: 'train',\n",
      "                 20: 'tvmonitor'},\n",
      "  'loss_objective': { '_target_': <class 'ssd.modeling.ssd_multibox_loss.SSDMultiboxLoss'>,\n",
      "                      'anchors': '${anchors}'},\n",
      "  'model': { '_target_': <class 'ssd.modeling.ssd.SSD300'>,\n",
      "             'anchors': '${anchors}',\n",
      "             'feature_extractor': { '_target_': <class 'ssd.modeling.backbones.vgg.VGG'>},\n",
      "             'loss_objective': '${loss_objective}',\n",
      "             'num_classes': 21},\n",
      "  'optimizer': { '_target_': <class 'torch.optim.adam.Adam'>,\n",
      "                 'betas': [0.9, 0.999],\n",
      "                 'lr': 0.005,\n",
      "                 'weight_decay': 0.0005},\n",
      "  'output_dir': PosixPath('outputs/configs/voc_vgg'),\n",
      "  'run_name': 'configs_voc_vgg',\n",
      "  'schedulers': { 'linear': { '_target_': <class 'torch.optim.lr_scheduler.LinearLR'>,\n",
      "                              'end_factor': 1,\n",
      "                              'start_factor': 0.1,\n",
      "                              'total_iters': 500},\n",
      "                  'multistep': { '_target_': <class 'torch.optim.lr_scheduler.MultiStepLR'>,\n",
      "                                 'gamma': 0.1,\n",
      "                                 'milestones': [70000, 9000]}},\n",
      "  'train': { '_output_dir': PosixPath('outputs'),\n",
      "             'amp': True,\n",
      "             'batch_size': 32,\n",
      "             'epochs': 40,\n",
      "             'image_channels': 3,\n",
      "             'imshape': [300, 300],\n",
      "             'log_interval': 15,\n",
      "             'seed': 0},\n",
      "  'train_cpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                           'transforms': [ { '_target_': <class 'ssd.data.transforms.transform.RandomSampleCrop'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.ToTensor'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.RandomHorizontalFlip'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.Resize'>,\n",
      "                                             'imshape': '${train.imshape}'},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.target_transform.GroundTruthBoxesToAnchors'>,\n",
      "                                             'anchors': '${anchors}',\n",
      "                                             'iou_threshold': 0.5}]},\n",
      "  'val_cpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                         'transforms': [ { '_target_': <class 'ssd.data.transforms.transform.ToTensor'>},\n",
      "                                         { '_target_': <class 'ssd.data.transforms.transform.Resize'>,\n",
      "                                           'imshape': '${train.imshape}'}]}}\n",
      "--------------------End of config file--------------------\n",
      "creating index...\n",
      "index created!\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "SSD300                        Parameters  Buffers  Output shape        Datatype\n",
      "---                           ---         ---      ---                 ---     \n",
      "feature_extractor.vgg.0       1792        -        [1, 64, 300, 300]   float32 \n",
      "feature_extractor.vgg.2       36928       -        [1, 64, 300, 300]   float32 \n",
      "feature_extractor.vgg.4       -           -        [1, 64, 150, 150]   float32 \n",
      "feature_extractor.vgg.5       73856       -        [1, 128, 150, 150]  float32 \n",
      "feature_extractor.vgg.7       147584      -        [1, 128, 150, 150]  float32 \n",
      "feature_extractor.vgg.9       -           -        [1, 128, 75, 75]    float32 \n",
      "feature_extractor.vgg.10      295168      -        [1, 256, 75, 75]    float32 \n",
      "feature_extractor.vgg.12      590080      -        [1, 256, 75, 75]    float32 \n",
      "feature_extractor.vgg.14      590080      -        [1, 256, 75, 75]    float32 \n",
      "feature_extractor.vgg.16      -           -        [1, 256, 38, 38]    float32 \n",
      "feature_extractor.vgg.17      1180160     -        [1, 512, 38, 38]    float32 \n",
      "feature_extractor.vgg.19      2359808     -        [1, 512, 38, 38]    float32 \n",
      "feature_extractor.vgg.21      2359808     -        [1, 512, 38, 38]    float32 \n",
      "feature_extractor.l2_norm     512         -        [1, 512, 38, 38]    float32 \n",
      "feature_extractor.vgg.23      -           -        [1, 512, 19, 19]    float32 \n",
      "feature_extractor.vgg.24      2359808     -        [1, 512, 19, 19]    float32 \n",
      "feature_extractor.vgg.26      2359808     -        [1, 512, 19, 19]    float32 \n",
      "feature_extractor.vgg.28      2359808     -        [1, 512, 19, 19]    float32 \n",
      "feature_extractor.extras.0.0  -           -        [1, 512, 19, 19]    float32 \n",
      "feature_extractor.extras.0.1  4719616     -        [1, 1024, 19, 19]   float32 \n",
      "feature_extractor.extras.0.2  -           -        [1, 1024, 19, 19]   float32 \n",
      "feature_extractor.extras.0.3  1049600     -        [1, 1024, 19, 19]   float32 \n",
      "feature_extractor.extras.0.4  -           -        [1, 1024, 19, 19]   float32 \n",
      "feature_extractor.extras.1.0  262400      -        [1, 256, 19, 19]    float32 \n",
      "feature_extractor.extras.1.1  -           -        [1, 256, 19, 19]    float32 \n",
      "feature_extractor.extras.1.2  1180160     -        [1, 512, 10, 10]    float32 \n",
      "feature_extractor.extras.1.3  -           -        [1, 512, 10, 10]    float32 \n",
      "feature_extractor.extras.2.0  65664       -        [1, 128, 10, 10]    float32 \n",
      "feature_extractor.extras.2.1  -           -        [1, 128, 10, 10]    float32 \n",
      "feature_extractor.extras.2.2  295168      -        [1, 256, 5, 5]      float32 \n",
      "feature_extractor.extras.2.3  -           -        [1, 256, 5, 5]      float32 \n",
      "feature_extractor.extras.3.0  32896       -        [1, 128, 5, 5]      float32 \n",
      "feature_extractor.extras.3.1  -           -        [1, 128, 5, 5]      float32 \n",
      "feature_extractor.extras.3.2  295168      -        [1, 256, 3, 3]      float32 \n",
      "feature_extractor.extras.3.3  -           -        [1, 256, 3, 3]      float32 \n",
      "feature_extractor.extras.4.0  32896       -        [1, 128, 3, 3]      float32 \n",
      "feature_extractor.extras.4.1  -           -        [1, 128, 3, 3]      float32 \n",
      "feature_extractor.extras.4.2  295168      -        [1, 256, 1, 1]      float32 \n",
      "feature_extractor.extras.4.3  -           -        [1, 256, 1, 1]      float32 \n",
      "regression_heads.0            73744       -        [1, 16, 38, 38]     float32 \n",
      "classification_heads.0        387156      -        [1, 84, 38, 38]     float32 \n",
      "regression_heads.1            221208      -        [1, 24, 19, 19]     float32 \n",
      "classification_heads.1        1161342     -        [1, 126, 19, 19]    float32 \n",
      "regression_heads.2            110616      -        [1, 24, 10, 10]     float32 \n",
      "classification_heads.2        580734      -        [1, 126, 10, 10]    float32 \n",
      "regression_heads.3            55320       -        [1, 24, 5, 5]       float32 \n",
      "classification_heads.3        290430      -        [1, 126, 5, 5]      float32 \n",
      "regression_heads.4            36880       -        [1, 16, 3, 3]       float32 \n",
      "classification_heads.4        193620      -        [1, 84, 3, 3]       float32 \n",
      "regression_heads.5            36880       -        [1, 16, 1, 1]       float32 \n",
      "classification_heads.5        193620      -        [1, 84, 1, 1]       float32 \n",
      "<top-level>:0                 34928       -        [1, 4, 8732]        float32 \n",
      "<top-level>:1                 -           -        [1, 4, 8732]        float32 \n",
      "---                           ---         ---      ---                 ---     \n",
      "Total                         26320414    0        -                   -       \n",
      "\n",
      "Epoch 0:   0%|                                          | 0/256 [00:00<?, ?it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python ../train.py ../configs/voc_vgg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf5864",
   "metadata": {},
   "source": [
    "# Useful commands:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397a7a7",
   "metadata": {},
   "source": [
    "#### Training and evaluation\n",
    "To start training:\n",
    "```\n",
    "python train.py  configs/ssd300.py\n",
    "```\n",
    "\n",
    "To starting training VGG on VOC:\n",
    "```\n",
    "python train.py  configs/voc_vgg.py\n",
    "```\n",
    "\n",
    "To only run evaluation:\n",
    "```\n",
    "python train.py  configs/ssd300.py --evaluate-only\n",
    "```\n",
    "\n",
    "#### Demo.py\n",
    "For VOC:\n",
    "```\n",
    "python demo.py configs/voc_vgg.py demo/voc demo/voc_output\n",
    "```\n",
    "\n",
    "For MNIST:\n",
    "```\n",
    "python demo.py configs/ssd300.py demo/mnist demo/mnist_output\n",
    "```\n",
    "\n",
    "\n",
    "#### Runtime analysis:\n",
    "```\n",
    "python3 runtime_analysis.py configs/ssd300.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
