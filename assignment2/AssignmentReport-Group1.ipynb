{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a) Part 1\n",
    "\n",
    "![task 1_1](T11.png)\n",
    "\n",
    "![task 1_2](T12.png)\n",
    "\n",
    "![task 1_3](T13.png)\n",
    "\n",
    "![task 1_4](T14.png)\n",
    "## task 1a) Part 2\n",
    "![task 1_5](T15.png)\n",
    "\n",
    "![task 1_6](T16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2c_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "i\\*h + h\\*o + i + o = 785\\*64 + 64\\*10 + 64 + 10 = 50954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a: Improve weight initialization\n",
    "We initialize the weight matrix using a normal distribution where we use a mean of 0 and a standard deviation of $\\frac{1}{\\sqrt{fan-in}}$ being \"fan-in\" = 785 in the hidden layer and 64 in the output layer.\n",
    "\n",
    "As we can see in the **Training and validation loss** figure the gap betweeen the validation and the training loss show us that the model with improved weights is more generalized, and as a consecuence has less overfiting.\n",
    "\n",
    "In the **Training and validation accuracy** figure we can see that the model with improved weights converge faster to a solution using less steps than the normal model and also it provides a more accurate solution.\n",
    "\n",
    "The normal model bump out at epoch 31 and the improved weights has a faster convergence bumping at the epoch 29.\n",
    "\n",
    "![task 3a loss](task3a_loss.png)\n",
    "\n",
    "![task 3a accuracy](task3a_acc.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b: Improve Sigmoid\n",
    "The new improvement was changing the sigmoid activation function in the hidden layers. The results of this function are not centered arround zero so we use the sigmoid activation function discussed by **LeCun**.\n",
    "\n",
    "For the forward activation we use the following sigmoid function:\n",
    "\n",
    "$f(x) = 1.7159 * tanh(\\frac{2}{3}x)$\n",
    "\n",
    "And for the backpropagation we use the following sigmoid function derivation:\n",
    "\n",
    "$f(x) = 1.7159 *\\frac{2}{3}* (1-tanh(\\frac{2}{3}*x^2))$\n",
    "\n",
    "As we can see the in the validation loss the sigmoid model validation loss the accuracy goes even further. But as we can appreciate the the is a large gap between the validation loss and the training loss, meaning that the model begging to overfit.\n",
    "\n",
    "This model has a improvement in speed and convergence rate in comparison with the other models.\n",
    "\n",
    "For this improvement we get a significant improvement in the number of epoch going from 29 to 8.\n",
    "\n",
    "![task 3b loss](task3b_loss.png)\n",
    "\n",
    "![task 3b accuracy](task3b_acc.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c: Momentum\n",
    "\n",
    "The last addition is adding the momentum for the gradient descent. Momentum makes that the gradient convergence faster. The momentum is given by:\n",
    "\n",
    "$∆w(t) = α * \\frac{∂C}{∂w} + γ∆w(t-1)$\n",
    "\n",
    "The training with all this show some overfiting. The validation loss is more that the improved sigmoid. The model is the fastest one. And the accuracy its almost perfect, this could be becouse the model was trained with a small data set.\n",
    "\n",
    "![task 3c loss](task3c_loss.png)\n",
    "\n",
    "![task 3c accuracy](task3c_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a,b)\n",
    "\n",
    "Here we compare the results of the network with different numbers of hidden layers. We use a 32 layer network, 64 and a 128. The base of our comparison is the 64 layer network because is the one that we use for the hole assignment.\n",
    "\n",
    "The 32 layer network perfoms worse than the 64 layer as we can see in due to the differeces between the training loss or the validation loss. The speed of the network also is prety slow. So we can say that a small number of hidden units will perform bad.\n",
    "\n",
    "The 128 layer network performs similary than the 64 layer network but it's slower in comparision. So The 128 layer network show that increasing the number of hidden units only provide a small improvement but it compromise the speed of the network.\n",
    "\n",
    "![Task 4ab loss](task4ab_loss.png)\n",
    "![Task 4ab accuracy](task4ab_acc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "Task 3: i\\*h + h\\*o + i + o = 785\\*64 + 64\\*10 + 64 + 10 = 50954\n"
    "Task 4d.1: i\\*h + h\\^HiddenLayersAmount + h\\*HiddenLayersAmount + h\\*o + i + o = 785\\*64 + 64\\^2 + 64\\*2 + 64\\*10 + 64 + 10\n"
    "Task 4d.2: i\\*h + h\\^HiddenLayersAmount + h\\*HiddenLayersAmount + h\\*o + i + o = 785\\*64 + 64\\^10 + 64\\*10 + 64\\*10 + 64 + 10\n"
    "The 4d.1 probabilly behaves better than the original one. 4d.2, despite having a greater amount of parameters, would perform worse (overfitting)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "There would be a better training loss and a worse validation loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 | packaged by conda-forge | (main, Jan 11 2023, 15:15:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "df634a05ea98b8e51c3f9b1ec71d257a08b6489e2fa4ba6256066b04a3fcb8c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
