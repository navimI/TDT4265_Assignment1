{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Whit the following input in the perfomance of a spacial convolution:\n",
    "| 2 | 1 | 2 | 3 | 1 |\n",
    "|---|---|---|---|---|\n",
    "| 4 | 5 | 0 | 7 | 0 |\n",
    "| 3 | 9 | 1 | 1 | 4 |\n",
    "\n",
    "and the following kernel:\n",
    "\n",
    "| -1 |  0 |  1 |\n",
    "|----|----|----|\n",
    "| -2 |  0 |  2 |\n",
    "| -1 |  0 |  1 |\n",
    "\n",
    "we add a zero-padding of size 1 to calculate the output of the convolution, having the following matrix:\n",
    "\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 0 | 2 | 1 | 2 | 3 | 1 | 0 |\n",
    "| 0 | 4 | 5 | 0 | 7 | 0 | 0 |\n",
    "| 0 | 3 | 9 | 1 | 1 | 4 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "For the fist row the calculations are the following:\n",
    "\n",
    "![task 1](task1a.png)\n",
    "\n",
    "If we do this for all the rows, we get the following matrix:\n",
    "\n",
    "|  7 | -4 |  6 | -2 | -13 |\n",
    "|----|----|----|----|-----|\n",
    "| 20 | -10| -2 |  2 | -18 |\n",
    "| 23 | -8 | -14|  6 | -9  |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Of the three given layers, the Max Pooling layer reduces the sensitivity to translational variations in the input. Max pooling does this by taking the maximum value over a spatial region in the input feature map. This means that if an object in an image is shifted slightly, its representation in the pooled feature map will remain largely unchanged.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "Assuming that the input image has dimensions H x W, and the kernel size is 7x7 with a stride of 1, the output shape can be calculated as follows:\n",
    "\n",
    "Output height = (H - kernel_size + 2*padding)/stride + 1\n",
    "\n",
    "Output width = (W - kernel_size + 2*padding)/stride + 1\n",
    "\n",
    "And as the height and width of the output image are the same, to keep the output shape (Height × Width) of the convolutional layer equal to the input image when using a stride of 1 and a kernel size of 7 × 7, you should use padding of size 3 on each side. This is because the total reduction in size due to the convolution operation is equal to (kernel_size - 1), which in this case is (7 - 1) = 6. Dividing this by two gives us the amount of padding needed on each side: 6/2 = 3.\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "We can use the following formula to calculate the spatial dimensions of the output feature map of a convolutional layer:\n",
    "\n",
    "Output size = (Input size - Filter size) / Stride + 1\n",
    "\n",
    "If the spatial dimensions of the feature maps in the first layer are 508 × 508, and no padding is used, then the input size to the first convolutional layer would be 512 × 512.\n",
    "\n",
    "Let's assume that the kernel size used in both convolutional layers is the same. We can use the above formula to calculate the kernel size:\n",
    "\n",
    "Output size = 508\n",
    "Input size = 512\n",
    "Stride = 1\n",
    "Filter size = ?\n",
    "\n",
    "508 = (512 - Filter size) / 1 + 1\n",
    "508 = (512 - Filter size) + 1\n",
    "508 - 1 = 512 - Filter size\n",
    "Filter size = 512 - 507 = 5\n",
    "\n",
    "Therefore, the spatial dimensions of the kernel used in both convolutional layers are 5 × 5.\n",
    "\n",
    "## task 1e)\n",
    "\n",
    "If subsampling is done after the first convolutional layer, using filters of size 2 × 2 and a stride of 2, then the spatial dimensions of the output feature maps will be reduced by a factor of 2 in each dimension.\n",
    "\n",
    "Since the spatial dimensions of the feature maps in the first convolutional layer are 508 × 508, the spatial dimensions of the pooled feature maps in the first pooling layer will be:\n",
    "\n",
    "Output size = (Input size - Filter size) / Stride + 1\n",
    "\n",
    "Output size = (508 - 2) / 2 + 1\n",
    "\n",
    "Output size = 254\n",
    "\n",
    "Therefore, the spatial dimensions of the pooled feature maps in the first pooling layer will be 254 × 254.\n",
    "\n",
    "## task 1f)\n",
    "\n",
    "the spatial dimensions of the feature maps in the second layer will be:\n",
    "\n",
    "Output size = (Input size - Filter size) / Stride + 1\n",
    "\n",
    "Output size = (254 - 3) / 1 + 1\n",
    "\n",
    "Output size = 252\n",
    "\n",
    "Therefore, the spatial dimensions of the feature maps in the second layer will be 252 × 252.\n",
    "## task 1g)\n",
    "\n",
    "To calculate the number of parameters in the network, we need to count the number of weights and biases in all layers. The weights in convolutional layers are the filters/kernels, and the biases in convolutional layers and fully-connected layers are separate learnable parameters.\n",
    "\n",
    "For each convolutional layer, the number of weights can be calculated as the product of the filter size, the number of input channels, and the number of filters. The number of biases is the same as the number of filters. The number of input channels for the first convolutional layer is 3 (for RGB images), and for subsequent convolutional layers, it is the number of filters in the previous layer.\n",
    "\n",
    "For the fully-connected layers, the number of weights can be calculated as the product of the number of input units and the number of output units. The number of biases is equal to the number of output units.\n",
    "\n",
    "Using this information, we can calculate the number of parameters in the network as follows:\n",
    "\n",
    "| Layer | Layer Type        | Number of Parameters                                                           |\n",
    "|-------|------------------|--------------------------------------------------------------------------------|\n",
    "| 1     | Conv2D            | 3 x 5 x 5 x 32 (weights) + 32 (biases) = 2,432                                   |\n",
    "| 1     | MaxPool2D         | 0                                                                              |\n",
    "| 2     | Conv2D            | 32 x 5 x 5 x 64 (weights) + 64 (biases) = 51,264                                 |\n",
    "| 2     | MaxPool2D         | 0                                                                              |\n",
    "| 3     | Conv2D            | 64 x 5 x 5 x 128 (weights) + 128 (biases) = 204,928                              |\n",
    "| 3     | MaxPool2D         | 0                                                                              |\n",
    "| 4     | Flatten           | 0                                                                              |\n",
    "| 4     | Fully-Connected   | 4 x 4 x 128 x 64 (weights) + 64 (biases) = 524,352                               |\n",
    "| 5     | Fully-Connected   | 64 x 10 (weights) + 10 (biases) = 650                                           |\n",
    "\n",
    "Therefore, the total number of parameters in the network is:\n",
    "\n",
    "2,432 + 51,264 + 204,928 + 524,352 + 650 = 783,626\n",
    "\n",
    "So the network has a total of 783,626 parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "![task 2](task2_plot.png)\n",
    "### Task 2b)\n",
    "\n",
    "| Dataset  | Accuracy |\n",
    "| -------- | -------- |\n",
    "| Training set | 87.1% |\n",
    "| Validation set  | 72.4% |\n",
    "| Test set  | 73.3% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "|Layer|Layer Type|Number of Hidden Units / Number of Filters|Activation Function|\n",
    "|-----|----------|---------|---------|\n",
    "|1|Conv2D|256|ReLU|\n",
    "|1|MaxPool2D|–|–|\n",
    "|2|Conv2D|512|ReLU|\n",
    "|2|MaxPool2D|–|–|\n",
    "|3|Conv2D|1024|ReLU|\n",
    "|3|MaxPool2D|–|–|\n",
    "|-----|----------|---------|---------|\n",
    "||Flatten|–|–|\n",
    "|4|Fully-Connected|512|ReLU|\n",
    "|5|Fully-Connected|10|Softmax|\n",
    "MaxPool2D's stride is set to 3.\n",
    "Other parameters and settings are the same as the original model.\n",
    "\n",
    "### Task 3b)\n",
    "![task 3b](task3b_plot.png)\n",
    "\n",
    "| Dataset | Loss | Accuracy |\n",
    "| -------- | -------- | -------- |\n",
    "| Training set | 0.38 | 87.6% |\n",
    "| Validation set | 0.74 | 76.4% |\n",
    "| Test set | 0.72 | 75.7% |\n",
    "\n",
    "Validation accuracy: 0.7712\n",
    "Validation loss: 0.7648\n",
    "Training loss: 0.2530\n",
    "### Task 3c)\n",
    "Increasing number of filters in the first convolunutional layer mainly did the boost, probabilly. More filters means more splitting over the raw data, so less information loss.\n",
    "Changing RElU function didn't help. Tried with some RElU variants, but no luck. Probabilly the main issue was they still RElU variants, similar functions.\n",
    "### Task 3d)\n",
    "![task 3d](task3d_plot.png)\n",
    "Accuracy: 0.7787\n",
    "### Task 3e)\n",
    "![task 3e](task3e_plot.png)\n",
    "Accuracy: 0.818\n",
    "### Task 3f)\n",
    "Validation loss stay constant over the epochs, so data are not overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "We will use in this task the pre-trained model, convined with our training, validation and test used in previous tasks. We use the SGD optimizer and the Adam (due to the hints aported in the assignment) optimizer to check the results.\n",
    "The results are that for the SGD optimizer we get:\n",
    "| Dataset | Loss | Accuracy |\n",
    "| -------- | -------- | -------- |\n",
    "| Training set | 0.52 | 82.7% |\n",
    "| Validation set | 0.48 | 84.3% |\n",
    "| Test set | 0.55 | 82.0% |\n",
    "\n",
    "Whit the following graph:\n",
    "\n",
    "![task 4SGD](task4_plot_SGD.png)\n",
    "\n",
    "As we can see the results are in less epochs that in the previous task. Also the precission is higher in comparison to the previous task.\n",
    "\n",
    "Whit the adam optimizer:\n",
    "\n",
    "| Dataset | Loss | Accuracy |\n",
    "| -------- | -------- | -------- |\n",
    "| Training set | 0.06 | 97.9% |\n",
    "| Validation set | 0.32 | 91.2% |\n",
    "| Test set | 0.34 | 89.4% |\n",
    "\n",
    "Whit the following graph:\n",
    "\n",
    "![task 4SGD](task4_plot_Adam.png)\n",
    "\n",
    "It solve the problem in less steps than in the SGD optimizer, but it start to overfiting the model, but with the early stopping, the accuracy reach the 89.4% in the test dataset. So it performs really well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "636a7b4364975c2b327292c177fa03c0858534c2e147176a51ac1930b55bb9fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
