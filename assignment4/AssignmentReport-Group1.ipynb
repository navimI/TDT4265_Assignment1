{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "The intersection over Union(IoU) is the measurement of the ratio between the predicted and the ground truth bounding boxes. The measurement is considered true positive if the result is overlaped by 50%. This evaluation is used in object detection task to measure the similarity between the predicted bounding boxes and the ground true bounding boxes.\n",
    "\n",
    "To calculate the IoU we have to calcualte the area of the intersection between the two bounding boxex, calculate the union between the two bounding boxes, and calculate the ratio between them.\n",
    "\n",
    "IoU = $\\frac{area(Bp \\cap Bgt)}{area(Bp \\cup Bgt)}$\n",
    "\n",
    "where $Bp$ is the predicted bounding box and $Bgt$ is the ground truth bounding box.\n",
    "\n",
    "![IoU](IoU.jpeg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Precision measures how accurate is your predictions. i.e. the percentage of your predictions are correct.\n",
    "\n",
    "\n",
    "\n",
    "Recall measures how good you find all the positives. For example, we can find 80% of the possible positive cases in our top K predictions.\n",
    "\n",
    "The precision and recall can be expressed mathematically as follows:\n",
    "\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "Recall = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "where $TP$ (true positive) is the number of correct positive predictions, $FP$ (false positive) is the number of incorrect positive predictions, and $FN$ (false negative) is the number of incorrect negative predictions.\n",
    "\n",
    "A true positive is a prediction that is correctly classified as positive, meaning that it belongs to the positive class and the model correctly identifies it as such. A false positive, on the other hand, is a prediction that is incorrectly classified as positive, meaning that it does not actually belong to the positive class but the model identifies it as such.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "Precision values for class 1 at different recall levels:\n",
    "\n",
    "Recall = 0.0: Precision = 1.0  \n",
    "Recall = 0.1: Precision = 1.0  \n",
    "Recall = 0.2: Precision = 1.0  \n",
    "Recall = 0.3: Precision = 1.0  \n",
    "Recall = 0.4: Precision = 1.0  \n",
    "Recall = 0.5: Precision = 1.0  \n",
    "Recall = 0.6: Precision = 1.0  \n",
    "Recall = 0.7: Precision = 0.5  \n",
    "Recall = 0.8 :Precision = 0.5  \n",
    "Recall = 0.9 :Precision = 0.5  \n",
    "Recall = 0.10 :Precision= 0.2  \n",
    "\n",
    "Area under the curve for class 1 AP = $\\frac{( 7 * 1 +3 * 0.5 + 0.2)}{11} $\n",
    "The solution is $AP = 0.7909$\n",
    "\n",
    "Precision values for class 2 at different recall levels:\n",
    "\n",
    "Recall = 0.0: Precision = 1.0  \n",
    "Recall = 0.1: Precision = 1.0  \n",
    "Recall = 0.2: Precision = 1.0  \n",
    "Recall = 0.3: Precision = 1.0  \n",
    "Recall = 0.4: Precision = 0.8  \n",
    "Recall = 0.5: Precision = 0.6  \n",
    "Recall = 0.6: Precision = 0.6  \n",
    "Recall = 0.7: Precision = 0.5  \n",
    "Recall = 0.8: Precision = 0.5  \n",
    "Recall = 0.9: Precision = 0.5  \n",
    "Recall = 1.0: Precision = 0.2\n",
    "\n",
    "Area under the curve for class  $AP = \\frac{( 4 * 1 + 0.8 + 2 * 0.6 + 3 * 0.5 + 0.2)}{11} $\n",
    "The solution is $AP = 0.7$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n",
    "![task 2f](task2/task2f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "The filtering operation to remove overlapping boxes in object detection with SSD (Single Shot Multibox Detector) is called Non-maximum Suppression (NMS). NMS is a post-processing technique commonly used in object detection to eliminate duplicate detections and improve the overall detection accuracy. It works by suppressing the boxes that have a high degree of overlap (measured using Intersection over Union) and keeping only the boxes with the highest confidence scores.\n",
    "\n",
    "### Task 3b)\n",
    "The statement is false. Predictions from the shallower layers in SSD are responsible for detecting small objects. Shallow layers can be used to predict small objects because small objects donâ€™t need bigger receptive fields.\n",
    "\n",
    "### Task 3c)\n",
    "This is used to be able to detect objects with different shapes and aspect ratios. For example, if we want to detect a car, we can use a square anchor box. However, if we want to detect a person, we can use a rectangular anchor box. This is because a person is not always in a square shape.\n",
    "\n",
    "### Task 3d)\n",
    "One of the main differences between SSD and YOLOv1/v2 that SSD is more accurate than YOLOv1/v2. SSD is more accurate because it uses a multi-scale feature map, which allows it to detect objects at different scales. YOLOv1/v2, on the other hand, uses a single-scale feature map, which means that it can only detect objects at one scale.\n",
    "\n",
    "### Task 3e)\n",
    "The total number of anchor boxes for this feature map, we can multiply the number of locations on the feature map (38x38) by the number of anchors per location (6). This gives us a total of 38 x 38 x 6 = 8664 anchor boxes for this feature map.\n",
    "\n",
    "### Task 3f)\n",
    "The total number of anchor boxes for the entire network, we can sum up the number of anchor boxes for each feature map. This gives us (38 x 38 + 19 x 19 + 10 x 10 + 5 x 5 + 3 x 3 +1 x1) x6 = 34956 anchor boxes in total for the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "![task 4b](task4b.png)\n",
    "mAP: 76.4% \n",
    "model trained on 20 epochs\n",
    "\n",
    "## Task 4c)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4d)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "FILL IN ANSWER. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
